{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b2507dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import regex as re\n",
    "from typing import Any, List, Dict, Tuple, Iterable, Iterator\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../../tests')\n",
    "from common import gpt2_bytes_to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f59bda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "reference_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b0ad0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, List, Dict, Tuple, Iterable, Iterator\n",
    "\n",
    "import regex as re\n",
    "\n",
    "PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: dict[int, bytes],\n",
    "        merges: list[tuple[bytes, bytes]],\n",
    "        special_tokens: list[str] | None = None,\n",
    "    ):\n",
    "        self.vocab=vocab\n",
    "        self.inv_vocab = {v:k for k,v in vocab.items()}\n",
    "\n",
    "        self.merges=merges\n",
    "        if isinstance(special_tokens, list) and special_tokens:\n",
    "            # Sort to ensure case ['<|eot|><|eot|>', '<|eot|>']\n",
    "            self.special_tokens=sorted(special_tokens, key=lambda x: (-len(x), x))\n",
    "            self.split_pat = re.compile(\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in self.special_tokens) + \")\"\n",
    "            )\n",
    "        else:\n",
    "            self.special_tokens=special_tokens\n",
    "            self.split_pat = None\n",
    "        \n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None):\n",
    "        with open(vocab_filepath) as f:\n",
    "            vocab = json.load(f)\n",
    "        \n",
    "        with open(merges_filepath) as f:\n",
    "            merges = [tuple(line.rstrip().split(\" \")) for line in f]\n",
    "        \n",
    "        tokenizer = cls(\n",
    "            vocab=vocab,\n",
    "            merges=merges,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "        return tokenizer\n",
    "    \n",
    "    def id_to_token(self, token_id: int) -> bytes:\n",
    "        return self.vocab[token_id]\n",
    "    \n",
    "    def token_to_id(self, token: bytes) -> int:\n",
    "        if token not in self.inv_vocab:\n",
    "            raise ValueError(\n",
    "                \"token {} not in vocab\".format(token.decode('utf-8', errors='ignore'))\n",
    "            )\n",
    "        return self.inv_vocab[token]\n",
    "        \n",
    "    \n",
    "    def merge(self, indices: List[bytes], merge_pair: Tuple[bytes, bytes]) -> List[bytes]:\n",
    "        merged_index = b''.join(merge_pair)\n",
    "        new_indices = []\n",
    "        \n",
    "        i=0\n",
    "        while i < len(indices):\n",
    "            if i+1 < len(indices):\n",
    "                pair = (indices[i], indices[i+1])\n",
    "                # pair = (bytes([indices[i]]), bytes([indices[i+1]]))\n",
    "                if pair==merge_pair:\n",
    "                    new_indices.append(merged_index)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_indices.append(indices[i])\n",
    "                    i+=1\n",
    "            else:\n",
    "                new_indices.append(indices[i])\n",
    "                i+=1\n",
    "        return new_indices\n",
    "    \n",
    "    def tokenize(self, text: str) -> list[bytes]:\n",
    "        if self.special_tokens:\n",
    "            parts = self.split_pat.split(text)\n",
    "        else:\n",
    "            parts = [text]\n",
    "        \n",
    "        if self.special_tokens:\n",
    "            parts = self.split_pat.split(text)\n",
    "        else:\n",
    "            parts = [text]\n",
    "        \n",
    "        indices = []\n",
    "        for part in parts:\n",
    "            # Handle special tokens\n",
    "            if self.special_tokens and part in self.special_tokens:\n",
    "                indices.append(part.encode('utf-8'))\n",
    "                continue\n",
    "            \n",
    "            for pretok_match in re.finditer(PAT, part):\n",
    "                pretok = pretok_match.group()\n",
    "                # Tokenize\n",
    "                part_bytes = pretok.encode('utf-8')\n",
    "                part_indices = list(map(lambda x: bytes([x]), part_bytes))\n",
    "                \n",
    "                # Merge\n",
    "                for merge_pair in self.merges:\n",
    "                    part_indices = self.merge(part_indices, merge_pair)\n",
    "            \n",
    "                indices.extend(part_indices)\n",
    "        return indices\n",
    "    \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokens = self.tokenize(text)\n",
    "        indices = [self.token_to_id(x) for x in tokens]\n",
    "        return indices\n",
    "             \n",
    "    \n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for x in iterable:\n",
    "            for token in self.encode(x):\n",
    "                yield token\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        tokens = [self.id_to_token(x) for x in ids]\n",
    "        return b''.join(tokens).decode('utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d953c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run pytest tests/test_tokenizer.py\n",
    "# adapter code\n",
    "def get_tokenizer(\n",
    "    vocab: dict[int, bytes],\n",
    "    merges: list[tuple[bytes, bytes]],\n",
    "    special_tokens: list[str] | None = None,\n",
    ") -> Any:\n",
    "    \"\"\"Given a vocabulary, a list of merges, and a list of special tokens,\n",
    "    return a BPE tokenizer that uses the provided vocab, merges, and special tokens.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict[int, bytes]): The tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "            to bytes (token bytes)\n",
    "        merges (list[tuple[bytes, bytes]]): BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "            representing that <token1> was merged with <token2>.\n",
    "            Merges are ordered by order of creation.\n",
    "        special_tokens (list[str] | None): A list of string special tokens for the tokenizer. These strings will never\n",
    "            be split into multiple tokens, and will always be kept as a single token.\n",
    "\n",
    "    Returns:\n",
    "        A BPE tokenizer that uses the provided vocab, merges, and special tokens.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10d73ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ac'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = (b'a', b'c')\n",
    "b''.join(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70816a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 b'a'\n",
      "98 b'b'\n",
      "99 b'c'\n",
      "b'a'\n",
      "b'b'\n",
      "b'c'\n"
     ]
    }
   ],
   "source": [
    "for x in \"abc\".encode('utf-8'):\n",
    "    print(x, bytes([x]))\n",
    "    \n",
    "part_bytes = 'abc'.encode('utf-8')\n",
    "for x in map(lambda x: bytes([x]), part_bytes):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "630d1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXTURES_PATH=\"../../tests/fixtures\"\n",
    "vocab_path = os.path.join(FIXTURES_PATH, \"gpt2_vocab.json\")\n",
    "merges_path = os.path.join(FIXTURES_PATH, \"gpt2_merges.txt\")\n",
    "\n",
    "special_tokens = ['<|endoftext|>']\n",
    "special_tokens = [\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69dbe36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2\n",
    "gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "with open(vocab_path) as vocab_f:\n",
    "    gpt2_vocab = json.load(vocab_f)\n",
    "gpt2_bpe_merges = []\n",
    "with open(merges_path) as f:\n",
    "    for line in f:\n",
    "        cleaned_line = line.rstrip()\n",
    "        if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "            gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "# The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "# just return the original bytes, so we don't force students to use\n",
    "# any particular encoding scheme.\n",
    "vocab = {\n",
    "    gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "    for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "}\n",
    "# If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "if special_tokens:\n",
    "    for special_token in special_tokens:\n",
    "        byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "        if byte_encoded_special_token not in set(vocab.values()):\n",
    "            vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "merges = [\n",
    "    (\n",
    "        bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "        bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "    )\n",
    "    for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc6a9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(\n",
    "    vocab=vocab,\n",
    "    merges=merges,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "# tokenizer = BPETokenizer.from_files(\n",
    "#     vocab_filepath=vocab_path,\n",
    "#     merges_filepath=merges_path,\n",
    "#     # special_tokens=[\"<|endoftext|>\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b86d8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex.Regex('(<\\\\|endoftext\\\\|><\\\\|endoftext\\\\|>|<\\\\|endoftext\\\\|>)', flags=regex.V0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens\n",
    "tokenizer.split_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f20e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.vocab#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8eb6646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'hello', b' world', b'<|endoftext|><|endoftext|>', b'he', b'he']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello world<|endoftext|><|endoftext|>hehe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a85b7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'hello', b' world', b'<|endoftext|>', b'he', b'he']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello world<|endoftext|>hehe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54fba901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373, 995, 50256, 258, 258]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello world<|endoftext|>hehe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bc2a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n",
      "b'\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# debugging test_encode_special_token_double_newline_non_whitespace\n",
    "print(tokenizer.id_to_token(198))\n",
    "print(tokenizer.id_to_token(628))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86e4bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>\\n\\ntesting!'\n"
     ]
    }
   ],
   "source": [
    "x = '''<|endoftext|>\n",
    "\n",
    "testing!'''\n",
    "print(repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "61eb6add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256, 198, 198, 33407, 0]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_tokenizer.encode(x, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db8c9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256, 198, 198, 33407, 0]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "62c2025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<regex.Match object; span=(0, 2), match='<|'>\n",
      "<regex.Match object; span=(2, 11), match='endoftext'>\n",
      "<regex.Match object; span=(11, 13), match='|>'>\n",
      "<regex.Match object; span=(13, 14), match='\\n'>\n",
      "<regex.Match object; span=(14, 15), match='\\n'>\n",
      "<regex.Match object; span=(15, 22), match='testing'>\n",
      "<regex.Match object; span=(22, 23), match='!'>\n"
     ]
    }
   ],
   "source": [
    "PAT=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "for match in re.finditer(PAT, x):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717c869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
